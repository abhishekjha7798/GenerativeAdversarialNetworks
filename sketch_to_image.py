# -*- coding: utf-8 -*-
"""Sketch_to_Image.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nDYYAMOjbKm055Aj7Mz5GpXdMwus77tu
"""

from __future__ import division
import math
import json
import random
import pprint
import scipy.misc
import numpy as np
from time import gmtime, strftime

import pdb

pp = pprint.PrettyPrinter()

get_stddev = lambda x, k_h, k_w: 1/math.sqrt(k_w*k_h*x.get_shape()[-1])

def get_image(image_path, image_size, is_crop=True):
    return transform(imread(image_path), image_size, is_crop)

def save_images(images, size, image_path):
    return imsave(inverse_transform(images), size, image_path)

def imread(path):
    return scipy.misc.imread(path).astype(np.float)

def merge_images(images, size):
    return inverse_transform(images)

def merge(images, size):
    h, w = images.shape[1], images.shape[2]
    img = np.zeros((int(h * size[0]), int(w * size[1]), 3))
    for idx, image in enumerate(images):
        i = idx % size[1]
        j = idx // size[1]
        img[j*h:j*h+h, i*w:i*w+w, :] = image

    return img

def imsave(images, size, path):
    return scipy.misc.imsave(path, merge(images, size))

def center_crop(x, crop_h, crop_w=None, resize_w=64):
    if crop_w is None:
        crop_w = crop_h
    h, w = x.shape[:2]
    j = int(round((h - crop_h)/2.))
    i = int(round((w - crop_w)/2.))
    return scipy.misc.imresize(x[j:j+crop_h, i:i+crop_w],
                               [resize_w, resize_w])

def transform(image, npx=64, is_crop=True):
    # npx : # of pixels width/height of image
    if is_crop:
        cropped_image = center_crop(image, npx)
    else:
        cropped_image = image
    return np.array(cropped_image)/127.5 - 1.

def inverse_transform(images):
    return (images+1.)/2.


def to_json(output_path, *layers):
    with open(output_path, "w") as layer_f:
        lines = ""
        for w, b, bn in layers:
            layer_idx = w.name.split('/')[0].split('h')[1]

            B = b.eval()

            if "lin/" in w.name:
                W = w.eval()
                depth = W.shape[1]
            else:
                W = np.rollaxis(w.eval(), 2, 0)
                depth = W.shape[0]

            biases = {"sy": 1, "sx": 1, "depth": depth, "w": ['%.2f' % elem for elem in list(B)]}
            if bn != None:
                gamma = bn.gamma.eval()
                beta = bn.beta.eval()

                gamma = {"sy": 1, "sx": 1, "depth": depth, "w": ['%.2f' % elem for elem in list(gamma)]}
                beta = {"sy": 1, "sx": 1, "depth": depth, "w": ['%.2f' % elem for elem in list(beta)]}
            else:
                gamma = {"sy": 1, "sx": 1, "depth": 0, "w": []}
                beta = {"sy": 1, "sx": 1, "depth": 0, "w": []}

            if "lin/" in w.name:
                fs = []
                for w in W.T:
                    fs.append({"sy": 1, "sx": 1, "depth": W.shape[0], "w": ['%.2f' % elem for elem in list(w)]})

                lines += """
                    var layer_%s = {
                        "layer_type": "fc",
                        "sy": 1, "sx": 1,
                        "out_sx": 1, "out_sy": 1,
                        "stride": 1, "pad": 0,
                        "out_depth": %s, "in_depth": %s,
                        "biases": %s,
                        "gamma": %s,
                        "beta": %s,
                        "filters": %s
                    };""" % (layer_idx.split('_')[0], W.shape[1], W.shape[0], biases, gamma, beta, fs)
            else:
                fs = []
                for w_ in W:
                    fs.append({"sy": 5, "sx": 5, "depth": W.shape[3], "w": ['%.2f' % elem for elem in list(w_.flatten())]})

                lines += """
                    var layer_%s = {
                        "layer_type": "deconv",
                        "sy": 5, "sx": 5,
                        "out_sx": %s, "out_sy": %s,
                        "stride": 2, "pad": 1,
                        "out_depth": %s, "in_depth": %s,
                        "biases": %s,
                        "gamma": %s,
                        "beta": %s,
                        "filters": %s
                    };""" % (layer_idx, 2**(int(layer_idx)+2), 2**(int(layer_idx)+2),
                             W.shape[0], W.shape[3], biases, gamma, beta, fs)
        layer_f.write(" ".join(lines.replace("'","").split()))

def make_gif(images, fname, duration=2, true_image=False):
  import moviepy.editor as mpy

  def make_frame(t):
    try:
      x = images[int(len(images)/duration*t)]
    except:
      x = images[-1]

    if true_image:
      return x.astype(np.uint8)
    else:
      return ((x+1)/2*255).astype(np.uint8)

  clip = mpy.VideoClip(make_frame, duration=duration)
  clip.write_gif(fname, fps = len(images) / duration)

def visualize(sess, dcgan, config, option):
  if option == 0:
    z_sample = np.random.uniform(-0.5, 0.5, size=(config.batch_size, dcgan.z_dim))
    samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})
    save_images(samples, [8, 8], './samples/test_%s.png' % strftime("%Y-%m-%d %H:%M:%S", gmtime()))
  elif option == 1:
    values = np.arange(0, 1, 1./config.batch_size)
    for idx in xrange(100):
      print(" [*] %d" % idx)
      z_sample = np.zeros([config.batch_size, dcgan.z_dim])
      for kdx, z in enumerate(z_sample):
        z[idx] = values[kdx]

      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})
      save_images(samples, [8, 8], './samples/test_arange_%s.png' % (idx))
  elif option == 2:
    values = np.arange(0, 1, 1./config.batch_size)
    for idx in [random.randint(0, 99) for _ in xrange(100)]:
      print(" [*] %d" % idx)
      z = np.random.uniform(-0.2, 0.2, size=(dcgan.z_dim))
      z_sample = np.tile(z, (config.batch_size, 1))
      #z_sample = np.zeros([config.batch_size, dcgan.z_dim])
      for kdx, z in enumerate(z_sample):
        z[idx] = values[kdx]

      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})
      make_gif(samples, './samples/test_gif_%s.gif' % (idx))
  elif option == 3:
    values = np.arange(0, 1, 1./config.batch_size)
    for idx in xrange(100):
      print(" [*] %d" % idx)
      z_sample = np.zeros([config.batch_size, dcgan.z_dim])
      for kdx, z in enumerate(z_sample):
        z[idx] = values[kdx]

      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})
      make_gif(samples, './samples/test_gif_%s.gif' % (idx))
  elif option == 4:
    image_set = []
    values = np.arange(0, 1, 1./config.batch_size)

    for idx in xrange(100):
      print(" [*] %d" % idx)
      z_sample = np.zeros([config.batch_size, dcgan.z_dim])
      for kdx, z in enumerate(z_sample): z[idx] = values[kdx]

      image_set.append(sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample}))
      make_gif(image_set[-1], './samples/test_gif_%s.gif' % (idx))

    new_image_set = [merge(np.array([images[idx] for images in image_set]), [10, 10]) \
        for idx in range(64) + range(63, -1, -1)]
    make_gif(new_image_set, './samples/test_gif_merged.gif', duration=8)


########## Elliott ##########
def get_text_batch(image_path, text_data):
    try:
        #pdb.set_trace()
        idx = int(image_path[0:6])-1
    except:
        print("image_path format is unexpected.")
    else:
        return text_data[idx]

def rgb2gray(rgb):
    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])

import math
import numpy as np
import tensorflow as tf

from tensorflow.python.framework import ops

from utils import *

class batch_norm(object):
    """Code modification of http://stackoverflow.com/a/33950177"""
    def __init__(self, epsilon=1e-5, momentum = 0.9, name="batch_norm"):
        with tf.variable_scope(name):
            self.epsilon = epsilon
            self.momentum = momentum

            self.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)
            self.name = name

    def __call__(self, x, train=True):
        shape = x.get_shape().as_list()

        if train:
            with tf.variable_scope(self.name) as scope:
                self.beta = tf.get_variable("beta", [shape[-1]],
                                    initializer=tf.constant_initializer(0.))
                self.gamma = tf.get_variable("gamma", [shape[-1]],
                                    initializer=tf.random_normal_initializer(1., 0.02))

                batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')
                with tf.variable_scope(tf.get_variable_scope(), reuse=False):
                    ema_apply_op = self.ema.apply([batch_mean, batch_var])
                    self.ema_mean, self.ema_var = self.ema.average(batch_mean), self.ema.average(batch_var)

                with tf.control_dependencies([ema_apply_op]):
                    mean, var = tf.identity(batch_mean), tf.identity(batch_var)
        else:
            mean, var = self.ema_mean, self.ema_var

        normed = tf.nn.batch_norm_with_global_normalization(
                x, mean, var, self.beta, self.gamma, self.epsilon, scale_after_normalization=True)

        return normed

def binary_cross_entropy(preds, targets, name=None):
    """Computes binary cross entropy given `preds`.
    For brevity, let `x = `, `z = targets`.  The logistic loss is
        loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))
    Args:
        preds: A `Tensor` of type `float32` or `float64`.
        targets: A `Tensor` of the same type and shape as `preds`.
    """
    eps = 1e-12
    with ops.op_scope([preds, targets], name, "bce_loss") as name:
        preds = ops.convert_to_tensor(preds, name="preds")
        targets = ops.convert_to_tensor(targets, name="targets")
        return tf.reduce_mean(-(targets * tf.log(preds + eps) +
                              (1. - targets) * tf.log(1. - preds + eps)))

def conv_cond_concat(x, y):
    """Concatenate conditioning vector on feature map axis."""
    x_shapes = x.get_shape()
    y_shapes = y.get_shape()
    return tf.concat(3, [x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])])

def conv2d(input_, output_dim,
           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,
           name="conv2d"):
    with tf.variable_scope(name):
        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],
                            initializer=tf.truncated_normal_initializer(stddev=stddev))
        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')

        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))
        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())
        conv = tf.nn.bias_add(conv, biases)

        return conv

def conv2d_transpose(input_, output_shape,
                     k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,
                     name="conv2d_transpose", with_w=False):
    with tf.variable_scope(name):
        # filter : [height, width, output_channels, in_channels]
        w = tf.get_variable('w', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]],
                            initializer=tf.random_normal_initializer(stddev=stddev))

        try:
            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,
                                strides=[1, d_h, d_w, 1])

        # Support for verisons of TensorFlow before 0.7.0
        except AttributeError:
            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,
                                strides=[1, d_h, d_w, 1])

        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))
        # deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
        deconv = tf.nn.bias_add(deconv, biases)

        if with_w:
            return deconv, w, biases
        else:
            return deconv

def lrelu(x, leak=0.2, name="lrelu"):
    with tf.variable_scope(name):
        f1 = 0.5 * (1 + leak)
        f2 = 0.5 * (1 - leak)
        return f1 * x + f2 * abs(x)

def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):
    shape = input_.get_shape().as_list()

    with tf.variable_scope(scope or "Linear"):
        matrix = tf.get_variable("Matrix", [shape[1], output_size], tf.float32,
                                 tf.random_normal_initializer(stddev=stddev))
        bias = tf.get_variable("bias", [output_size],
            initializer=tf.constant_initializer(bias_start))
        if with_w:
            return tf.matmul(input_, matrix) + bias, matrix, bias
        else:
            return tf.matmul(input_, matrix) + bias

        
######## Elliott ########        
def kl_divergence(p, q):
    tf.assert_rank(p,2)
    tf.assert_rank(q,2)
    
    p_shape = tf.shape(p)
    q_shape = tf.shape(q)
    tf.assert_equal(p_shape, q_shape)
    
    # normalize sum to 1
    p_ = tf.divide(p, tf.tile(tf.expand_dims(tf.reduce_sum(p,axis=1), 1), [1,p_shape[1]]))
    q_ = tf.divide(q, tf.tile(tf.expand_dims(tf.reduce_sum(q,axis=1), 1), [1,p_shape[1]]))
    
    return tf.reduce_sum(tf.multiply(p_, tf.log(tf.divide(p_, q_))), axis=1)

# Commented out IPython magic to ensure Python compatibility.
from __future__ import division
import os
import time
from glob import glob
import tensorflow as tf
import pickle
from six.moves import xrange
from scipy.stats import entropy

from ops import *
from utils import *

#import pdb

class GAN(object):
    def __init__(self, sess, image_size=64, is_crop=False,
                 batch_size=64, text_vector_dim=100,
                 z_dim=100, t_dim=256, gf_dim=64, df_dim=64, c_dim=3,
                 checkpoint_dir=None, sample_dir=None, log_dir=None, 
                 lam1=0.1, lam2=0.1, lam3=0.1):
        """
        Args:
            sess: TensorFlow session
            batch_size: The size of batch. Should be specified before training.
            z_dim: (optional) Dimension of dim for Z. [100]
            t_dim: (optional) Dimension of text features. [256]
            gf_dim: (optional) Dimension of gen filters in first conv layer. [64]
            df_dim: (optional) Dimension of discrim filters in first conv layer. [64]
            c_dim: (optional) Dimension of image color. [3]
            lam1: (optional) Hyperparameter for contextual loss. [0.1]
            lam2: (optional) Hyperparameter for perceptual loss. [0.1]
            lam3: (optional) Hyperparameter for wrong examples [0.1]
        """
        self.sess = sess
        self.is_crop = is_crop
        self.batch_size = batch_size
        self.text_vector_dim = text_vector_dim
        self.image_size = image_size
        self.image_shape = [image_size, image_size * 2, 3]
        # self.image_shape = [image_size, image_size, 3]

        self.sample_freq = int(100*64/batch_size)
        self.save_freq = int(500*64/batch_size)

        self.z_dim = z_dim
        self.t_dim = t_dim

        self.gf_dim = gf_dim
        self.df_dim = df_dim

        self.lam1 = lam1
        self.lam2 = lam2
        self.lam3 = lam3

        self.c_dim = 3

        # batch normalization : deals with poor initialization helps gradient flow
        self.d_bn1 = batch_norm(name='d_bn1')
        self.d_bn2 = batch_norm(name='d_bn2')
        self.d_bn3 = batch_norm(name='d_bn3')
        self.d_bn4 = batch_norm(name='d_bn4')

        self.g_bn0 = batch_norm(name='g_bn0')
        self.g_bn1 = batch_norm(name='g_bn1')
        self.g_bn2 = batch_norm(name='g_bn2')
        self.g_bn3 = batch_norm(name='g_bn3')

        self.checkpoint_dir = checkpoint_dir
        self.sample_dir = sample_dir
        self.log_dir = log_dir
        
        self.build_model()

        self.model_name = "GAN"

        
    def build_model(self):
        self.images = tf.placeholder(
            tf.float32, [self.batch_size] + self.image_shape, name='real_images')
        self.sample_images= tf.placeholder(
            tf.float32, [self.batch_size] + self.image_shape, name='sample_images')

        self.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim], name='z')
        self.z_sum = tf.summary.histogram("z", self.z)

        self.t = tf.placeholder(tf.float32, [self.batch_size, self.text_vector_dim], name='t')
        self.t_sum = tf.summary.histogram("t", self.t)

        self.t_wr = tf.placeholder(tf.float32, [self.batch_size, self.text_vector_dim], name='t_wr')
        self.t_wr_sum = tf.summary.histogram("t_wr", self.t_wr)

        #self.images_wr = tf.placeholder(
        #    tf.float32, [self.batch_size] + self.image_shape, name='wrong_images')

        self.G = self.generator(self.z, self.t)
        self.D_rl, self.D_logits_rl = self.discriminator(self.images, self.t)
        self.D_fk, self.D_logits_fk = self.discriminator(self.G, self.t, reuse=True)
        self.D_wr, self.D_logits_wr = self.discriminator(self.images, self.t_wr, reuse=True)

        self.sampler = self.sampler(self.z, self.t)

        self.G_sum = tf.image_summary("G", self.G)
        self.d_rl_sum = tf.summary.histogram("d", self.D_rl)
        self.d_fk_sum = tf.summary.histogram("d_", self.D_fk)
        self.d_wr_sum = tf.summary.histogram("d_wr", self.D_wr)

        # cross entropy loss
        self.g_loss = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_fk,
                                                    tf.ones_like(self.D_fk)))
        self.d_loss_real = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_rl,
                                                    tf.ones_like(self.D_rl)))
        self.d_loss_fake = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_fk,
                                                    tf.zeros_like(self.D_fk)))
        self.d_loss_wrong = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_wr,
                                                    tf.zeros_like(self.D_wr)))

        '''
        # least square loss
        self.d_loss_real = 0.5 * tf.reduce_mean((self.D_logits_rl - tf.ones_like(self.D_logits_rl))**2)
        self.d_loss_fake = 0.5 * tf.reduce_mean((self.D_logits_fk - tf.zeros_like(self.D_logits_fk))**2)
        self.d_loss_wrong = 0.5 * tf.reduce_mean((self.D_logits_wr - tf.zeros_like(self.D_logits_wr))**2)
        self.g_loss = 0.5 * tf.reduce_mean((self.D_logits_fk - tf.ones_like(self.D_logits_fk))**2)
        '''

        self.d_loss_real_sum = tf.scalar_summary("d_loss_real", self.d_loss_real)
        self.d_loss_fake_sum = tf.scalar_summary("d_loss_fake", self.d_loss_fake)
        self.d_loss_wrong_sum = tf.scalar_summary("d_loss_wrong", self.d_loss_wrong)

        self.d_loss = self.d_loss_real + self.d_loss_fake + self.lam3 * self.d_loss_wrong

        self.g_loss_sum = tf.scalar_summary("g_loss", self.g_loss)
        self.d_loss_sum = tf.scalar_summary("d_loss", self.d_loss)

        t_vars = tf.trainable_variables()

        self.d_vars = [var for var in t_vars if 'd_' in var.name]
        self.g_vars = [var for var in t_vars if 'g_' in var.name]

        self.saver = tf.train.Saver(max_to_keep=50)

        # mask to generate
        self.mask = tf.placeholder(tf.float32, [None] + self.image_shape, name='mask')
        
        # l1
        #self.contextual_loss = tf.reduce_sum(
        #    tf.contrib.layers.flatten(
        #        tf.abs(tf.mul(self.mask, self.G) - tf.mul(self.mask, self.images))), 1)
        
        # kl divergence
        self.contextual_loss = kl_divergence(
            tf.divide(tf.add(tf.contrib.layers.flatten(tf.image.rgb_to_grayscale(
                tf.slice(self.G, [0,0,0,0], [self.batch_size,self.image_size,self.image_size,self.c_dim]))), 1), 2),
            tf.divide(tf.add(tf.contrib.layers.flatten(tf.image.rgb_to_grayscale(
                tf.slice(self.images, [0,0,0,0], [self.batch_size,self.image_size,self.image_size,self.c_dim]))), 1), 2))
        
        self.perceptual_loss = self.g_loss
        self.complete_loss = self.lam1*self.contextual_loss + self.lam2*self.perceptual_loss
        self.grad_complete_loss = tf.gradients(self.complete_loss, self.z)

        
    def train(self, config):
        image_data = glob(os.path.join(config.dataset, "*.png"))
        #np.random.shuffle(data)
        print (os.path.join(config.dataset, "*.png"))
        assert(len(image_data) > 0)
        
        text_data = pickle.load(open(config.text_path, 'rb'))
        
        ######### for face attributes #########
        attr_sum = np.sum(text_data, 0)
        attr_percent = (1 + attr_sum/len(text_data)) / 2
        print ("selected attribute percentages:\n", attr_percent)
        ######### for face attributes #########
        
        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \
                          .minimize(self.d_loss, var_list=self.d_vars)
        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \
                          .minimize(self.g_loss, var_list=self.g_vars)
        tf.initialize_all_variables().run()

        self.g_sum = tf.merge_summary(
            [self.z_sum, self.t_sum, self.d_fk_sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])
        self.d_sum = tf.merge_summary(
            [self.z_sum, self.t_sum, self.d_rl_sum, self.d_wr_sum, self.d_loss_real_sum, self.d_loss_wrong_sum, self.d_loss_sum])
        self.writer = tf.train.SummaryWriter(self.log_dir, self.sess.graph)

        
        #++++++++ training sample ++++++++#
        
        sample_z = np.random.uniform(-1, 1, size=(self.batch_size , self.z_dim))
        sample_files = image_data[0:self.batch_size]
        sample = [get_image(sample_file, self.image_size, is_crop=self.is_crop) for sample_file in sample_files]
        sample_images = np.array(sample).astype(np.float32)
        sample_t_ = [get_text_batch(os.path.basename(sample_file), text_data) for sample_file in sample_files]
        sample_t = np.array(sample_t_).astype(np.float32)
        nRows = np.ceil(self.batch_size/8)
        nCols = min(8, self.batch_size) #8

        ######### for face attributes #########
        with open(os.path.join(self.sample_dir, 'sampled_texts.txt'), 'wb') as f:
            np.savetxt(f, sample_t, fmt='%i', delimiter='\t')
        ######### for face attributes #########

        #-------- training sample --------#

        counter = 1
        start_time = time.time()

        if self.load(self.checkpoint_dir):
            print("""
============
An existing model was found in the checkpoint directory.
If you just cloned this repository, it's Brandon Amos'
trained model for faces that's used in the post.
If you want to train a new model from scratch,
delete the checkpoint directory or specify a different
--checkpoint_dir argument.
============
""")
        else:
            print("""
============
An existing model was not found in the checkpoint directory.
Initializing a new one.
============
""")

        for epoch in xrange(config.epoch):
            image_data = glob(os.path.join(config.dataset, "*.png"))
            batch_idxs = min(len(image_data), config.train_size) // self.batch_size

            for idx in xrange(0, batch_idxs):
                
                #++++++++ data loading ++++++++#
                
                data_start_time = time.time()
                batch_files = image_data[idx*config.batch_size:(idx+1)*config.batch_size]
                batch = [get_image(batch_file, self.image_size, is_crop=self.is_crop)
                         for batch_file in batch_files]
                batch_images = np.array(batch).astype(np.float32)

                batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \
                            .astype(np.float32)

                batch_t_ = [get_text_batch(os.path.basename(batch_file), text_data)
                         for batch_file in batch_files]
                batch_t = np.array(batch_t_).astype(np.float32)

                ######### for face attributes #########
                # randomly generated wrong face attributes
                batch_t_wr_ = [np.random.choice(np.arange(2), size=config.batch_size, 
                                                p=[1-attr_percent[i], attr_percent[i]]) * 2 - 1 
                               for i in xrange(self.text_vector_dim)]
                batch_t_wr = np.transpose(batch_t_wr_).astype(np.float32)
                ######### for face attributes #########
                
                '''
                # randomly select wrong images
                idx_wr = np.random.randint(batch_idxs)
                while (idx_wr == idx):
                    idx_wr = np.random.randint(batch_idxs)
                batch_files_wr = image_data[idx_wr*config.batch_size:(idx_wr+1)*config.batch_size]
                batch_wr = [get_image(batch_file_wr, self.image_size, is_crop=self.is_crop)
                         for batch_file_wr in batch_files_wr]
                batch_images_wr = np.array(batch_wr).astype(np.float32)
                '''
                data_time = time.time() - data_start_time

                #-------- data loading --------#

                
                #++++++++ training ++++++++#
                
                # Update D network
                _, summary_str = self.sess.run([d_optim, self.d_sum],
                    feed_dict={ self.images: batch_images, self.z: batch_z, self.t: batch_t, self.t_wr: batch_t_wr })
                self.writer.add_summary(summary_str, counter)

                # Update G network
                _, summary_str = self.sess.run([g_optim, self.g_sum],
                    feed_dict={ self.z: batch_z, self.t: batch_t })
                self.writer.add_summary(summary_str, counter)

                # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)
                _, summary_str = self.sess.run([g_optim, self.g_sum],
                    feed_dict={ self.z: batch_z, self.t: batch_t })
                self.writer.add_summary(summary_str, counter)
                
                errD_fake = self.d_loss_fake.eval({self.z: batch_z, self.t: batch_t})
                errD_real = self.d_loss_real.eval({self.images: batch_images, self.t: batch_t})
                errG = self.g_loss.eval({self.z: batch_z, self.t: batch_t})
                #-------- training --------#

                counter += 1
                print("Epoch: [%2d] [%4d/%4d] data_time: %4.4f, time: %4.4f, d_loss: %.8f, g_loss: %.8f" \
#                     % (epoch, idx, batch_idxs, data_time,
                        time.time() - start_time, errD_fake+errD_real, errG))
                
                if np.mod(counter, self.sample_freq) == 1:
                    samples = self.sess.run(
                        [self.sampler], feed_dict={self.z: sample_z, self.t: sample_t})
                    save_images(samples[0], [nRows, nCols],
                                os.path.join(self.sample_dir, 'train_{:02d}_{:04d}.png'.format(epoch, idx)))
                    #print("[Sample] d_loss: %.8f, g_loss: %.8f" % (d_loss, g_loss))

                if np.mod(counter, self.save_freq) == 2:
                    self.save(config.checkpoint_dir, counter)


    def test(self, config):
        tf.initialize_all_variables().run()

        isLoaded = self.load(self.checkpoint_dir)
        assert(isLoaded)

        # image_data = glob(os.path.join(config.dataset, "*.png"))
        nImgs = len(config.imgs)

        batch_idxs = int(np.ceil(nImgs/self.batch_size))
        if config.maskType == 'right':
            mask = np.ones(self.image_shape)
            mask[:,self.image_size:,:] = 0.0
        elif config.maskType == 'left':
            mask = np.ones(self.image_shape)
            mask[:,:self.image_size,:] = 0.0
        else:
            assert(False)
            
        text_data = pickle.load(open(config.text_path, 'rb'))

        num_batch = int(np.ceil(nImgs/self.batch_size))
        for idx in xrange(0, num_batch):
            print('batch no. ' + str(idx+1) + ':\n')
            
            l = idx*self.batch_size
            u = min((idx+1)*self.batch_size, nImgs)
            batchSz = u-l
            batch_files = config.imgs[l:u]
            batch = [get_image(batch_file, self.image_size, is_crop=self.is_crop)
                     for batch_file in batch_files]
            batch_images = np.array(batch).astype(np.float32)
            batch_mask = np.resize(mask, [self.batch_size] + self.image_shape)
            
            os.makedirs(os.path.join(config.outDir, 'hats_imgs_{:04d}'.format(idx)))
            os.makedirs(os.path.join(config.outDir, 'completed_{:04d}'.format(idx)))
            
            #++++++++ for face attributes ++++++++#
            
            # attributes to be loaded
            if (config.attributes[0] == None):
                batch_t_ = [get_text_batch(os.path.basename(batch_file), text_data)
                        for batch_file in batch_files]
                batch_t = np.array(batch_t_).astype(np.float32)
                
            # user_defiened attributes
            else:
                attr_v = config.attributes
                assert(len(attr_v) == self.text_vector_dim, "attribute vector must have the given length")
                print('using attributes: ', attr_v)
                batch_t = np.array([attr_v,]*batchSz).astype(np.float32)
                
            with open(os.path.join(config.outDir, 'completed_{:04d}/texts.txt'.format(idx)), 'wb') as f:
                np.savetxt(f, batch_t, fmt='%i', delimiter='\t')
                
            #-------- for face attributes --------#
            
            # last batch
            if batchSz < self.batch_size:
                print(batchSz)
                padSz = ((0, int(self.batch_size-batchSz)), (0,0), (0,0), (0,0))
                batch_images = np.pad(batch_images, padSz, 'wrap')
                batch_images = batch_images.astype(np.float32)
                batch_t = np.pad(batch_t, ((0, int(self.batch_size-batchSz)), (0,0)), 'wrap')
            
            
            nRows = np.ceil(batchSz/8)
            nCols = min(8, batchSz) #8
            
            
            #++++++++ z initialization ++++++++#
            
            zhats_init = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim)).astype(np.float32)
            zhats_ = zhats_init.copy()
            kl_div = np.full(len(zhats_), np.inf)
            in_flat = [rgb2gray(img[:,:self.image_size,:]).flatten() for img in batch_images]
            in_flat = np.array(in_flat) + 1
            kld_avg = 0
            
            kld_f = open(os.path.join(config.outDir, 'hats_imgs_{:04d}/kld_init.txt'.format(idx)), 'w')
            kld_f.write('average kl divergence of initializations:')
            for i in xrange(30):
                G_imgs = self.sess.run([self.G], feed_dict={ self.z: zhats_, self.t: batch_t })
                save_images(G_imgs[0][:batchSz,:,:,:], [nRows, nCols],
                        os.path.join(config.outDir, 'hats_imgs_{:04d}/init_{:02d}.png'.format(idx, i)))
                
                out_flat = [rgb2gray(img[:,:self.image_size,:]).flatten() for img in G_imgs[0]]
                out_flat = np.array(out_flat) + 1
                
                # choose lowest kl divergence
                for j in xrange(self.batch_size):
                    kl_d = entropy(in_flat[j], out_flat[j])
                    if (kl_d < kl_div[j]):
                        zhats_init[j] = zhats_[j]
                        kl_div[j] = kl_d
                
                kld_avg = kl_div.mean()
                print('average KL divergence:', kld_avg)
                kld_f.write('{:02d}: {:04.4f}'.format(i, kld_avg))
                
                zhats_ = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim)).astype(np.float32)
            
            print('choosing min KL divergence:', kld_avg)
            kld_f.write('choosing min KL divergence: {:04.4f}'.format(kld_avg))
            kld_f.close()
            
            G_imgs = self.sess.run([self.G], feed_dict={ self.z: zhats_init, self.t: batch_t })
            save_images(G_imgs[0][:batchSz,:,:,:], [nRows, nCols],
                        os.path.join(config.outDir, 'hats_imgs_{:04d}/chosen_init.png'.format(idx)))
            
            #-------- z initialization --------#
            
            
            #++++++++ completion ++++++++#
            
            zhats = zhats_init.copy().astype(np.float32)
            v = 0

            save_images(batch_images[:batchSz,:,:,:], [nRows,nCols],
                        os.path.join(config.outDir, 'hats_imgs_{:04d}/gt.png'.format(idx)))
            masked_images = np.multiply(batch_images, batch_mask)
            save_images(masked_images[:batchSz,:,:,:], [nRows,nCols],
                        os.path.join(config.outDir, 'hats_imgs_{:04d}/masked.png'.format(idx)))
            
            for i in xrange(config.nIter):
                fd = {
                    self.z: zhats,
                    self.mask: batch_mask,
                    self.images: batch_images,
                    self.t: batch_t,
                }
                run = [self.complete_loss, self.grad_complete_loss, self.G]
                loss, g, G_imgs = self.sess.run(run, feed_dict=fd)

                # update zhats
                v_prev = np.copy(v)
                v = config.momentum*v - config.lr*g[0]
                zhats += -config.momentum * v_prev + (1+config.momentum)*v
                zhats = np.clip(zhats, -1, 1)

                # save images
                if i % 20 == 0:
                    print(i, np.mean(loss[0:batchSz]))
                    imgName = os.path.join(config.outDir,
                                           'hats_imgs_{:04d}/{:04d}.png'.format(idx, i))
                    save_images(G_imgs[:batchSz,:,:,:], [nRows,nCols], imgName)

                    inv_masked_hat_images = np.multiply(G_imgs, 1.0-batch_mask)
                    completed = masked_images + inv_masked_hat_images
                    imgName = os.path.join(config.outDir,
                                           'completed_{:04d}/{:04d}.png'.format(idx, i))
                    save_images(completed[:batchSz,:,:,:], [nRows,nCols], imgName)
                    
            #-------- completion --------#


            #++++++++ interpolation visualization ++++++++#
            
            zhats_final = np.copy(zhats)
            diff = zhats_final - zhats_init
            step = 5
            for i in xrange(step):
                z_ = zhats_init + diff / (step-1) * i
                G_imgs = self.sess.run([self.G], feed_dict={ self.z: z_, self.t: batch_t })
                imgName = os.path.join(config.outDir, 'hats_imgs_{:04d}/{:01d}_interp.png'.format(idx, i))
                save_images(G_imgs[0][:batchSz,:,:,:], [nRows,nCols], imgName)
                
            #-------- interpolation visualization --------#
                
                    
    def discriminator(self, image, t, reuse=False):
        if reuse:
            tf.get_variable_scope().reuse_variables()

        h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))
        
        t_ = tf.expand_dims(t, 1)
        t_ = tf.expand_dims(t_, 2)
        t_tiled = tf.tile(t_, [1,32,64,1], name='tiled_t')
        h0_concat = tf.concat(3, [h0, t_tiled], name='h0_concat')
        
        h1 = lrelu(self.d_bn1(conv2d(h0_concat, self.df_dim*2, name='d_h1_conv')))
        h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))
        h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv')))

        #h4 = linear(tf.reshape(h3, [-1, 8192*2]), 1, 'd_h3_lin')
        # conv to 512x1x1
        h4 = conv2d(h3, self.df_dim*8, 4, 8, 1, 1, name='d_h4_conv')

        return tf.nn.sigmoid(h4), h4

    
    def generator(self, z, t):
        
        self.z_, self.h0_lin_w, self.h0_lin_b = linear(z, self.gf_dim*4*8, 'g_h0_lin', with_w=True)
        z_ = tf.reshape(self.z_, [-1, 4, 8, self.gf_dim])
        
        t_ = tf.expand_dims(tf.expand_dims(t, 1), 2)
        t_tiled = tf.tile(t_, [1,4,8,1])
        
        h0_concat = tf.concat(3, [z_, t_tiled])
        
        self.h0, self.h0_w, self.h0_b = conv2d_transpose(h0_concat,
            [self.batch_size, 4, 8, self.gf_dim*8], 1, 1, 1, 1, name='g_h0', with_w=True)
        h0 = tf.nn.relu(self.g_bn0(self.h0))
        
        self.h1, self.h1_w, self.h1_b = conv2d_transpose(h0,
            [self.batch_size, 8, 16, self.gf_dim*4], name='g_h1', with_w=True)
        h1 = tf.nn.relu(self.g_bn1(self.h1))

        h2, self.h2_w, self.h2_b = conv2d_transpose(h1,
            [self.batch_size, 16, 32, self.gf_dim*2], name='g_h2', with_w=True)
        h2 = tf.nn.relu(self.g_bn2(h2))

        h3, self.h3_w, self.h3_b = conv2d_transpose(h2,
            [self.batch_size, 32, 64, self.gf_dim*1], name='g_h3', with_w=True)
        h3 = tf.nn.relu(self.g_bn3(h3))

        h4, self.h4_w, self.h4_b = conv2d_transpose(h3,
            [self.batch_size, 64, 128, 3], name='g_h4', with_w=True)

        return tf.nn.tanh(h4)

    
    def sampler(self, z, t, y=None):
        tf.get_variable_scope().reuse_variables()

        z_ = tf.reshape(linear(z, self.gf_dim*4*8, 'g_h0_lin'), [-1, 4, 8, self.gf_dim])
        
        t_ = tf.expand_dims(tf.expand_dims(t, 1), 2)
        t_tiled = tf.tile(t_, [1,4,8,1])
        
        h0_concat = tf.concat(3, [z_, t_tiled])
        
        h0 = conv2d_transpose(h0_concat, 
            [self.batch_size, 4, 8, self.gf_dim*8], 1, 1, 1, 1, name='g_h0')
        h0 = tf.nn.relu(self.g_bn0(h0, train=False))

        h1 = conv2d_transpose(h0, [self.batch_size, 8, 16, self.gf_dim*4], name='g_h1')
        h1 = tf.nn.relu(self.g_bn1(h1, train=False))

        h2 = conv2d_transpose(h1, [self.batch_size, 16, 32, self.gf_dim*2], name='g_h2')
        h2 = tf.nn.relu(self.g_bn2(h2, train=False))

        h3 = conv2d_transpose(h2, [self.batch_size, 32, 64, self.gf_dim*1], name='g_h3')
        h3 = tf.nn.relu(self.g_bn3(h3, train=False))

        h4 = conv2d_transpose(h3, [self.batch_size, 64, 128, 3], name='g_h4')

        return tf.nn.tanh(h4)


    def save(self, checkpoint_dir, step):
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        self.saver.save(self.sess,
                        os.path.join(checkpoint_dir, self.model_name),
                        global_step=step)

        
    def load(self, checkpoint_dir):
        print(" [*] Reading checkpoints...")

        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            self.saver.restore(self.sess, ckpt.model_checkpoint_path)
            return True
        else:
            return False

import os
import scipy.misc
import numpy as np
import tensorflow as tf

from model import GAN

flags = tf.app.flags
flags.DEFINE_integer("epoch", 25, "Epoch to train [25]")
flags.DEFINE_float("learning_rate", 0.0002, "Learning rate of for adam [0.0002]")
flags.DEFINE_float("beta1", 0.5, "Momentum term of adam [0.5]")
flags.DEFINE_integer("train_size", np.inf, "The size of train images [np.inf]")
flags.DEFINE_integer("batch_size", 64, "The size of batch images [64]")
flags.DEFINE_integer("image_size", 64, "The size of image to use [64]")
flags.DEFINE_integer("text_vector_dim", 100, "The dimension of input text vector [100]")
flags.DEFINE_string("dataset", "datasets/celeba/train", "Dataset directory.")
flags.DEFINE_string("checkpoint_dir", "checkpoint", "Directory name to save the checkpoints [checkpoint]")
flags.DEFINE_string("sample_dir", "samples", "Directory name to save the image samples [samples]")
flags.DEFINE_string("log_dir", "logs", "Directory name to save the logs [logs]")
flags.DEFINE_string("text_path", "text_embeddings.pkl", "Path of the text embeddings [text_embeddings.pkl]")
#flags.DEFINE_float("lam1", 0.1, "Hyperparameter for contextual loss [0.1]")
#flags.DEFINE_float("lam2", 0.1, "Hyperparameter for perceptual loss [0.1]")
flags.DEFINE_float("lam3", 0.1, "Hyperparameter for wrong examples [0.1]")
FLAGS = flags.FLAGS

if not os.path.exists(FLAGS.checkpoint_dir):
    os.makedirs(FLAGS.checkpoint_dir)
if not os.path.exists(FLAGS.sample_dir):
    os.makedirs(FLAGS.sample_dir)

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
with tf.Session(config=config) as sess:
    model = GAN(sess, 
                  image_size=FLAGS.image_size, 
                  batch_size=FLAGS.batch_size, 
                  text_vector_dim=FLAGS.text_vector_dim,
                  checkpoint_dir=FLAGS.checkpoint_dir, 
                  sample_dir=FLAGS.sample_dir, 
                  log_dir=FLAGS.log_dir, 
                  is_crop=False, 
                  lam3=FLAGS.lam3,
                 )

    model.train(FLAGS)

import argparse
import os
import tensorflow as tf

from model import GAN

parser = argparse.ArgumentParser()
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--momentum', type=float, default=0.9)
parser.add_argument('--nIter', type=int, default=1000)
parser.add_argument('--imgSize', type=int, default=64)
parser.add_argument('--batchSize', type=int, default=64)
parser.add_argument('--text_vector_dim', type=int, default=100)
parser.add_argument('--lam1', type=float, default=0.1) # Hyperparameter for contextual loss [0.1]
parser.add_argument('--lam2', type=float, default=0.1) # Hyperparameter for perceptual loss [0.1]
#parser.add_argument('--lam3', type=float, default=0.1) # Hyperparameter for wrong example [0.1]
parser.add_argument('--checkpointDir', type=str, default='checkpoint')
parser.add_argument('--outDir', type=str, default='results')
parser.add_argument('--text_path', type=str, default='text_embeddings.pkl')
parser.add_argument('--maskType', type=str,
                    choices=['random', 'center', 'left', 'right', 'full'],
                    default='right')
parser.add_argument('--attributes', nargs='+', type=int, default=[None])
parser.add_argument('imgs', type=str, nargs='+')

args = parser.parse_args()

assert(os.path.exists(args.checkpointDir))

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
with tf.Session(config=config) as sess:
    model = GAN(sess, 
                image_size=args.imgSize, 
                batch_size=args.batchSize, 
                text_vector_dim=args.text_vector_dim,
                checkpoint_dir=args.checkpointDir, 
                lam1=args.lam1, 
                lam2=args.lam2,
               )
    model.test(args)